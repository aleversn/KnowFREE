# Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations

![workflow](docs/assets/workflow.png)

### ğŸŒ Overview

This repository provides the official implementation of our paper:

> **Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations**
> [arXiv:2501.19093](https://arxiv.org/abs/2501.19093)

Low-resource sequence labeling often suffers from data sparsity and limited contextual generalization.
We propose **KnowFREE (Knowledge-Fused Representation Enhancement Framework)** â€” a framework that integrates **external linguistic knowledge** and **contextual label explanations** into the modelâ€™s representation space to enhance low-resource performance.

**Key Highlights:**

Combining an **LLM-based knowledge enhancement workflow** with a **span-based KnowFREE model** to effectively address these challenges.

**Pipeline 1: Label Extension Annotation**
* Objective: To leverage LLMs to generate extension entity labels, word segmentation tags, and POS tags for the original samples.
* Effect:
  * Enhances the model's understanding of fine-grained contextual semantics.
  * Improves the ability to distinguish entity boundaries in character-dense languages.

**Pipeline 2: Enriched Explanation Synthesis**

* Objective: Using LLMs to generate detailed, context-aware explanations for target entities, thereby synthesizing new, high-quality training samples.
* Effect:
  * Effectively mitigates semantic distribution bias between synthetic samples and the target domain.
  * Significantly expands the number of samples and improves model performance in extremely low-resource settings.



---

### ğŸ”— Quick Links

  - [Model Checkpoints](#â™ ï¸-model-checkpoints)
  - [Data Augmentation Workflow](#ğŸ“Š-data-augmentation-workflow)
  - [Train KnowFREE](#ğŸ”¥-run-knowfree-models)
  - [Citation](#ğŸ“š-citation)

### â™ ï¸ Model Checkpoints

Due to the large number of experiments, the architectural differences between the initial and reconstructed models, and the limited practical value of low-resource checkpoints sampled from the full dataset, we only release a few representative checkpoints (e.g., weibo) on Hugging Face for reference, as shown below:

| Model                                                                                                                         |  F1   |
| :---------------------------------------------------------------------------------------------------------------------------- | :---: |
| [aleversn/KnowFREE-Weibo-BERT-base (Many shots 1000 with ChatGLM3)](https://huggingface.co/aleversn/KnowFREE-Weibo-BERT-base) | 76.78 |
| [aleversn/KnowFREE-Youku-BERT-base (Many shots 1000 with ChatGLM3)](https://huggingface.co/aleversn/KnowFREE-Youku-BERT-base) | 84.50 |

---

### ğŸ§© KnowFREE Framework

![KnowFREE](docs/assets/knowfree.png)

**Architecture**: A Biaffine-based span model that supports **nested entity** annotation.

**Core Innovations:**

* Introduces a **Local Multi-head Attention Layer** to efficiently fuse the multi-type extension label features generated in Pipeline 1.
* **No External Knowledge Needed for Inference:** The model learns to fuse knowledge during the training, the logits of extension labels will be masked during inference.

---

### âš™ï¸ Installation Guide

#### Core Dependencies

Create an environment and install dependencies:

```bash
conda create -n knowfree python=3.8
conda activate knowfree
```

```bash
pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html
pip install transformers==4.18.0 fastNLP==1.0.1 PrettyTable
pip install torch-scatter==2.0.8 -f https://data.pyg.org/whl/torch-1.8.0+cu111.html
```

### ğŸ“Š Data Augmentation Workflow

See the detailed data synthesis pipeline in [Syn_Pipelines](docs/Syn_Pipelines.md).

In KnowFREE, we employ **contextual paraphrasing and label explanation synthesis** to augment low-resource datasets.
For each entity label, LLMs generate descriptive explanations that are integrated into the learning process to mitigate label semantic sparsity.

---

### ğŸ”¥ Run KnowFREE Models

#### Training with `KnowFREE`

##### Dataset Format

Specify the dataset path using the `data_present_path` argument (`Default`: `./datasets/present.json`). The file should be a JSON object with the following format:

```json
{
    "weibo": {
        "train": "./datasets/weibo/train.jsonl",
        "dev": "./datasets/weibo/dev.jsonl",
        "test": "./datasets/weibo/test.jsonl",
        "labels": "./datasets/weibo/labels.txt"
    }
}
```

**Train Samples of Different Languages:**

- Chinese

```jsonl
{"text": ["ç§‘", "æŠ€", "å…¨", "æ–¹", "ä½", "èµ„", "è®¯", "æ™º", "èƒ½", "ï¼Œ", "å¿«", "æ·", "çš„", "æ±½", "è½¦", "ç”Ÿ", "æ´»", "éœ€", "è¦", "æœ‰", "ä¸‰", "å±", "ä¸€", "äº‘", "çˆ±", "ä½ "], "label": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "entities": []}
{"text": ["å¯¹", "ï¼Œ", "è¾“", "ç»™", "ä¸€", "ä¸ª", "å¥³", "äºº", "ï¼Œ", "çš„", "æˆ", "ç»©", "ã€‚", "å¤±", "æœ›"], "label": ["O", "O", "O", "O", "O", "O", "B-PER.NOM", "E-PER.NOM", "O", "O", "O", "O", "O", "O", "O"], "entities": [{"start": 6, "entity": "PER.NOM", "end": 8, "text": ["å¥³", "äºº"]}]}
{"text": ["ä»Š", "å¤©", "ä¸‹", "åˆ", "èµ·", "æ¥", "çœ‹", "åˆ°", "å¤–", "é¢", "çš„", "å¤ª", "é˜³", "ã€‚", "ã€‚", "ã€‚", "ã€‚", "æˆ‘", "ç¬¬", "ä¸€", "å", "åº”", "ç«Ÿ", "ç„¶", "æ˜¯", "å¼º", "çƒˆ", "çš„", "æƒ³", "å›", "å®¶", "æ³ª", "æƒ³", "æˆ‘", "ä»¬", "ä¸€", "èµ·", "åœ¨", "å˜‰", "é±¼", "ä¸ª", "æ—¶", "å€™", "äº†", "ã€‚", "ã€‚", "ã€‚", "ã€‚", "æœ‰", "å¥½", "å¤š", "å¥½", "å¤š", "çš„", "è¯", "æƒ³", "å¯¹", "ä½ ", "è¯´", "æ", "å·¾", "å‡¡", "æƒ³", "è¦", "ç˜¦", "ç˜¦", "ç˜¦", "æˆ", "æ", "å¸†", "æˆ‘", "æ˜¯", "æƒ³", "åˆ‡", "å¼€", "äº‘", "æœµ", "çš„", "å¿ƒ"], "label": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-LOC.NAM", "E-LOC.NAM", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-PER.NAM", "I-PER.NAM", "E-PER.NAM", "O", "O", "O", "O", "O", "O", "B-PER.NAM", "E-PER.NAM", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "entities": [{"start": 38, "entity": "LOC.NAM", "end": 40, "text": ["å˜‰", "é±¼"]}, {"start": 59, "entity": "PER.NAM", "end": 62, "text": ["æ", "å·¾", "å‡¡"]}, {"start": 68, "entity": "PER.NAM", "end": 70, "text": ["æ", "å¸†"]}]}
```

- English

```jsonl
{"text": ["im", "thinking", "of", "a", "comedy", "where", "a", "group", "of", "husbands", "receive", "one", "chance", "from", "their", "wives", "to", "engage", "with", "other", "women"], "entities": [{"start": 4, "end": 5, "entity": "GENRE", "text": ["comedy"]}, {"start": 6, "end": 21, "entity": "PLOT", "text": ["a", "group", "of", "husbands", "receive", "one", "chance", "from", "their", "wives", "to", "engage", "with", "other", "women"]}]}
{"text": ["another", "sequel", "of", "an", "action", "movie", "about", "drag", "street", "car", "races", "alcohol", "and", "gun", "violence"], "entities": [{"start": 1, "end": 2, "entity": "RELATIONSHIP", "text": ["sequel"]}, {"start": 4, "end": 5, "entity": "GENRE", "text": ["action"]}, {"start": 7, "end": 15, "entity": "PLOT", "text": ["drag", "street", "car", "races", "alcohol", "and", "gun", "violence"]}]}
{"text": ["what", "is", "the", "name", "of", "the", "movie", "in", "which", "a", "group", "of", "criminals", "begin", "to", "suspect", "that", "one", "of", "them", "is", "a", "police", "informant", "after", "a", "simple", "jewelery", "heist", "goes", "terribly", "wrong"], "entities": [{"start": 9, "end": 32, "entity": "PLOT", "text": ["a", "group", "of", "criminals", "begin", "to", "suspect", "that", "one", "of", "them", "is", "a", "police", "informant", "after", "a", "simple", "jewelery", "heist", "goes", "terribly", "wrong"]}]}
{"text": ["a", "movie", "with", "vin", "diesel", "in", "world", "war", "2", "in", "a", "foreign", "country", "shooting", "people"], "entities": [{"start": 3, "end": 5, "entity": "ACTOR", "text": ["vin", "diesel"]}, {"start": 6, "end": 9, "entity": "GENRE", "text": ["world", "war", "2"]}, {"start": 11, "end": 15, "entity": "PLOT", "text": ["foreign", "country", "shooting", "people"]}]}
{"text": ["what", "is", "the", "1991", "disney", "animated", "movie", "that", "featured", "angela", "lansbury", "as", "the", "voice", "of", "a", "teapot"], "entities": [{"start": 3, "end": 4, "entity": "YEAR", "text": ["1991"]}, {"start": 5, "end": 6, "entity": "GENRE", "text": ["animated"]}, {"start": 9, "end": 11, "entity": "ACTOR", "text": ["angela", "lansbury"]}, {"start": 16, "end": 17, "entity": "CHARACTER_NAME", "text": ["teapot"]}]}
```

- Japanese

```jsonl
{"text": ["I", "n", "f", "o", "r", "m", "i", "x", "ã®", "å‹•", "ã", "ã‚’", "ã¿", "ã¦", "ã€", "ã‚ª", "ãƒ©", "ã‚¯", "ãƒ«", "ã¨", "I", "B", "M", "ã‚‚", "è¿½", "éš", "ã—", "ãŸ", "ã€‚"], "entities": [{"start": 0, "end": 8, "entity": "æ³•äººå", "text": ["I", "n", "f", "o", "r", "m", "i", "x"]}, {"start": 15, "end": 19, "entity": "æ³•äººå", "text": ["ã‚ª", "ãƒ©", "ã‚¯", "ãƒ«"]}, {"start": 20, "end": 23, "entity": "æ³•äººå", "text": ["I", "B", "M"]}]}
{"text": ["ç¾", "åœ¨", "ã¯", "ã‚¢", "ãƒ‹", "ãƒ¡", "ãƒ¼", "ã‚·", "ãƒ§", "ãƒ³", "æ¥­", "ç•Œ", "ã‹", "ã‚‰", "é€€", "ã„", "ã¦", "ãŠ", "ã‚Š", "ã€", "æ°´", "å½©", "ç”»", "å®¶", "ã¨", "ã—", "ã¦", "ã‚‚", "æ´»", "å‹•", "ã—", "ã¦", "ã„", "ã‚‹", "ã€‚"], "entities": []}
{"text": ["å¤§", "é‡", "æ±", "ã‚¤", "ãƒ³", "ã‚¿", "ãƒ¼", "ãƒ", "ã‚§", "ãƒ³", "ã‚¸", "ã¯", "ã€", "å¤§", "åˆ†", "çœŒ", "è±Š", "å¾Œ", "å¤§", "é‡", "å¸‚", "å¤§", "é‡", "ç”º", "å¾Œ", "ç”°", "ã«", "ã‚", "ã‚‹", "ä¸­", "ä¹", "å·", "æ¨ª", "æ–­", "é“", "è·¯", "ã®", "ã‚¤", "ãƒ³", "ã‚¿", "ãƒ¼", "ãƒ", "ã‚§", "ãƒ³", "ã‚¸", "ã§", "ã‚", "ã‚‹", "ã€‚"], "entities": [{"start": 0, "end": 11, "entity": "æ–½è¨­å", "text": ["å¤§", "é‡", "æ±", "ã‚¤", "ãƒ³", "ã‚¿", "ãƒ¼", "ãƒ", "ã‚§", "ãƒ³", "ã‚¸"]}, {"start": 13, "end": 26, "entity": "åœ°å", "text": ["å¤§", "åˆ†", "çœŒ", "è±Š", "å¾Œ", "å¤§", "é‡", "å¸‚", "å¤§", "é‡", "ç”º", "å¾Œ", "ç”°"]}, {"start": 29, "end": 36, "entity": "æ–½è¨­å", "text": ["ä¸­", "ä¹", "å·", "æ¨ª", "æ–­", "é“", "è·¯"]}]}
{"text": ["2", "0", "1", "4", "å¹´", "1", "æœˆ", "1", "5", "æ—¥", "ã€", "ãƒ", "ãƒ", "ã‚¿", "ã¯", "ãƒŸ", "ãƒ£", "ãƒ³", "ãƒ", "ãƒ¼", "ã®", "ä¸Š", "åº§", "éƒ¨", "ä»", "æ•™", "ã‚’", "æ“", "è­·", "ã™", "ã‚‹", "ä½¿", "å‘½", "ã‚’", "æŒ", "ã£", "ã¦", "ã€", "ãƒ", "ãƒ³", "ãƒ€", "ãƒ¬", "ãƒ¼", "ã®", "ä»", "æ•™", "åƒ§", "ã®", "å¤§", "è¦", "æ¨¡", "ãª", "ä¼š", "è­°", "ã§", "æ­£", "å¼", "ã«", "è¨­", "ç«‹", "ã•", "ã‚Œ", "ãŸ", "ã€‚"], "entities": [{"start": 11, "end": 14, "entity": "æ³•äººå", "text": ["ãƒ", "ãƒ", "ã‚¿"]}, {"start": 15, "end": 20, "entity": "åœ°å", "text": ["ãƒŸ", "ãƒ£", "ãƒ³", "ãƒ", "ãƒ¼"]}, {"start": 38, "end": 43, "entity": "åœ°å", "text": ["ãƒ", "ãƒ³", "ãƒ€", "ãƒ¬", "ãƒ¼"]}]}
{"text": ["æ°¸", "æ³°", "è˜", "é§…", "ã¯", "ã€", "ä¸­", "è¯", "äºº", "æ°‘", "å…±", "å’Œ", "å›½", "åŒ—", "äº¬", "å¸‚", "æµ·", "æ·€", "åŒº", "ã«", "ä½", "ç½®", "ã™", "ã‚‹", "åŒ—", "äº¬", "åœ°", "ä¸‹", "é‰„", "8", "å·", "ç·š", "ã®", "é§…", "ã§", "ã‚", "ã‚‹", "ã€‚"], "entities": [{"start": 0, "end": 4, "entity": "æ–½è¨­å", "text": ["æ°¸", "æ³°", "è˜", "é§…"]}, {"start": 6, "end": 19, "entity": "åœ°å", "text": ["ä¸­", "è¯", "äºº", "æ°‘", "å…±", "å’Œ", "å›½", "åŒ—", "äº¬", "å¸‚", "æµ·", "æ·€", "åŒº"]}]}
```

- Korean

```jsonl
{"text": ["ê·¸", "ëª¨ìŠµ", "ì„", "ë³´", "ã„´", "ë¯¼ì´", "ëŠ”", "í• ì•„ë²„ì§€", "ê°€", "ë§ˆì¹˜", "ì „ìŸí„°", "ì—ì„œ", "ì´ê¸°", "ê³ ", "ëŒì•„ì˜¤", "ã„´", "ì¥êµ°", "ì²˜ëŸ¼", "ì˜ì “", "í•˜", "ì•„", "ë³´ì´", "ã„´ë‹¤ê³ ", "ìƒê°", "í•˜", "ì•˜", "ìŠµë‹ˆë‹¤", "."], "entities": [{"start": 5, "end": 6, "entity": "PS", "text": ["ë¯¼ì´"]}]}
{"text": ["ë‚´ë‹¬", "18", "ì¼", "ë¶€í„°", "ë‚´ë…„", "2", "ì›”", "20", "ì¼", "ê¹Œì§€", "ëŠ”", "ì„œìš¸ì—­", "ì—ì„œ", "ë¬´ì£¼ë¦¬ì¡°íŠ¸", "ë¶€ê·¼", "ê¹Œì§€", "ìŠ¤í‚¤ê´€ê´‘", "ì—´ì°¨", "ë¥¼", "ìš´í–‰", "í•˜", "ã„´ë‹¤", "."], "entities": [{"start": 0, "end": 10, "entity": "DT", "text": ["ë‚´ë‹¬", "18", "ì¼", "ë¶€í„°", "ë‚´ë…„", "2", "ì›”", "20", "ì¼", "ê¹Œì§€"]}, {"start": 11, "end": 12, "entity": "LC", "text": ["ì„œìš¸ì—­"]}, {"start": 13, "end": 14, "entity": "OG", "text": ["ë¬´ì£¼ë¦¬ì¡°íŠ¸"]}]}
{"text": ["í˜¸ì†Œë ¥", "ìˆ", "ê³ ", "ì„ ë™", "ì ", "ì´", "ã„´", "ì£¼ì œ", "ë¥¼", "ì¡ì•„ë‚´", "ëŠ”", "ë°", "ëŠ¥í•˜", "ã„´", "ì¦ˆìœ…", "ì´", "ì§€ë§Œ", "ì´", "ì˜í™”", "ì—ì„œ", "ëŠ”", "ë¬´ì—‡", "ì´", "í˜¸ì†Œë ¥", "ì´", "ìˆ", "ì„ì§€", "ê²°ì •", "í•˜", "ì§€", "ëª»í•˜", "ê³ ", "ë§ì„¤ì´", "ã„´ë‹¤", "."], "entities": [{"start": 14, "end": 15, "entity": "PS", "text": ["ì¦ˆìœ…"]}]}
{"text": ["ê·¸ë˜ì„œ", "ì„¸í˜¸", "ëŠ”", "ë°¤", "ì´", "ë©´", "ì¹œêµ¬", "ë„¤", "ì§‘", "ì„", "ëŒì•„ë‹¤ë‹ˆ", "ë©°", "ì•„ë²„ì§€", "ëª°ë˜", "ì—°ìŠµ", "ì„", "í•˜", "ì•˜", "ìŠµë‹ˆë‹¤", "."], "entities": [{"start": 1, "end": 2, "entity": "PS", "text": ["ì„¸í˜¸"]}, {"start": 3, "end": 4, "entity": "TI", "text": ["ë°¤"]}]}
{"text": ["í™©ì”¨", "ëŠ”", "ìì‹ ", "ì´", "ì–´ë¦¬", "ì–´ì„œ", "ë“£", "ì€", "ì´", "ì´ì•¼ê¸°", "ê°€", "ì–´ë¦°ì´", "ë“¤", "ì—ê²Œ", "ì†Œë°•", "í•˜", "ã„´", "íš¨ì", "ì˜", "ë§ˆìŒ", "ì„", "ì „í•˜", "ì•„", "ì£¼", "ã„¹", "ìˆ˜", "ìˆ", "ì„", "ê²ƒ", "ê°™", "ì•„", "5", "ë¶„", "ì§œë¦¬", "êµ¬ì—°ë™í™”", "ë¡œ", "ê°ìƒ‰", "í•˜", "ì•˜", "ë‹¤ê³ ", "ë§", "í•˜", "ã„´ë‹¤", "."], "entities": [{"start": 0, "end": 1, "entity": "PS", "text": ["í™©ì”¨"]}, {"start": 31, "end": 33, "entity": "TI", "text": ["5", "ë¶„"]}]}
{"text": ["ì•„ë²„ì§€", "ê°€", "ëŒì•„ê°€", "ì‹œ", "ã„´", "ë’¤", "ì–´ë¨¸ë‹ˆ", "ì˜", "í¸ì• ", "ë¥¼", "ë°°ê²½", "ìœ¼ë¡œ", "ìŠ¹ì£¼", "ëŠ”", "ì§‘ì•ˆ", "ì—ì„œ", "ë§Œ", "ì€", "ëŒ€ë‹¨", "í•˜", "ã„´", "ê¶Œì„¸", "ë¥¼", "ëˆ„ë¦¬", "ì—ˆ", "ë‹¤", "."], "entities": [{"start": 12, "end": 13, "entity": "PS", "text": ["ìŠ¹ì£¼"]}]}
```

**Labels**

- `.txt`

```
O
GPE.NAM
GPE.NOM
LOC.NAM
LOC.NOM
ORG.NAM
ORG.NOM
PER.NAM
PER.NOM
```

- `.json` / `.jsonl`

```json
{
    "O": {
        "idx": 0,
        "count": -1,
        "is_target": true
    },
    "GPE.NAM": {
        "idx": 1,
        "count": -1,
        "is_target": true
    },
    "GPE.NOM": {
        "idx": 2,
        "count": -1,
        "is_target": true
    },
    "LOC.NAM": {
        "idx": 3,
        "count": -1,
        "is_target": true
    },
    "LOC.NOM": {
        "idx": 4,
        "count": -1,
        "is_target": true
    },
    "ORG.NAM": {
        "idx": 5,
        "count": -1,
        "is_target": true
    },
    "ORG.NOM": {
        "idx": 6,
        "count": -1,
        "is_target": true
    },
    "PER.NAM": {
        "idx": 7,
        "count": -1,
        "is_target": true
    },
    "PER.NOM": {
        "idx": 8,
        "count": -1,
        "is_target": true
    },
    "ADJECTIVE": {
        "idx": 9,
        "count": 1008,
        "is_target": false
    },
    "ADPOSITION": {
        "idx": 10,
        "count": 41,
        "is_target": false
    },
    "ADVERB": {
        "idx": 11,
        "count": 1147,
        "is_target": false
    },
    "APP": {
        "idx": 12,
        "count": 3,
        "is_target": false
    },
    "AUXILIARY": {
        "idx": 13,
        "count": 4,
        "is_target": false
    },...
}
```

* **Model**: BERT / RoBERTa

```python
from main.trainers.knowfree_trainer import Trainer
from transformers import BertTokenizer, BertConfig

MODEL_PATH = "<MODEL_PATH>"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
config = BertConfig.from_pretrained(MODEL_PATH)
trainer = Trainer(tokenizer=tokenizer, config=config, from_pretrained=MODEL_PATH,
                  data_name='<DATASET_NAME>',
                  batch_size=4,
                  batch_size_eval=8,
                  task_name='<TASK_NAME>')

for i in trainer(num_epochs=120, other_lr=1e-3, weight_decay=0.01, remove_clashed=True, nested=False, eval_call_step=lambda x: x % 125 == 0):
    a = i
```

**Key Params**

- `other_lr`: the learning rate of the non-PLM part.
- `remove_clashed`: remove the label that exists overlap (only choose the label with min start position)
- `nested`: whether support nested entities, when do sequence labeling like `CMeEE`, you should set it as true and disabled `remove_clashed`.
- `eval_call_step`: determine evaluation with `x` steps, defined with a function call.

##### Evaluation Only

Comment out the training loop to evaluate directly:

```python
trainer.eval(0, is_eval=True)
```

#### Train with `CNN Nested NER`

```python
from main.trainers.cnnner_trainer import Trainer
from transformers import BertTokenizer, BertConfig

MODEL_PATH = "<MODEL_PATH>"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
config = BertConfig.from_pretrained(MODEL_PATH)
trainer = Trainer(tokenizer=tokenizer, config=config, from_pretrained=MODEL_PATH,
                  data_name='<DATASET_NAME>',
                  batch_size=4,
                  batch_size_eval=8,
                  task_name='<TASK_NAME>')

for i in trainer(num_epochs=120, other_lr=1e-3, weight_decay=0.01, remove_clashed=True, nested=False, eval_call_step=lambda x: x % 125 == 0):
    a = i
```

##### Prediction

```python
from main.predictor.knowfree_predictor import KnowFREEPredictor
from transformers import BertTokenizer, BertConfig

MODEL_PATH = "<MODEL_PATH>"
LABEL_FILE = '<LABEL_PATH>'
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
config = BertConfig.from_pretrained(MODEL_PATH)
pred = KnowFREEPredictor(tokenizer=tokenizer, config=config, from_pretrained=MODEL_PATH, label_file=LABEL_FILE, batch_size=4)

for entities in pred(['å¶èµŸè‘†ï¼šå…¨çƒæ—¶å°šè´¢è¿æ»šæ»šè€Œæ¥é’±', 'æˆ‘è¦å»æˆ‘è¦å»èŠ±å¿ƒèŠ±å¿ƒèŠ±å¿ƒè€¶åˆ†æ‰‹å¤§å¸ˆè´µä»”é‚“è¶…å››å¤§åæ•å›´è§‚è¯ç­’è½¬å‘é‚“è¶…è´´å§å¾®åšå·å¤–è¯ç­’æœ›å‘¨çŸ¥ã€‚é‚“è¶…å››å¤§åæ•']):
    print(entities)
```

**Result**

```json
[
    [
        {'start': 0, 'end': 3, 'entity': 'PER.NAM', 'text': ['å¶', 'èµŸ', 'è‘†'
            ]
        }
    ],
    [
        {'start': 45, 'end': 47, 'entity': 'PER.NAM', 'text': ['é‚“', 'è¶…'
            ]
        },
        {'start': 19, 'end': 21, 'entity': 'PER.NAM', 'text': ['é‚“', 'è¶…'
            ]
        },
        {'start': 31, 'end': 33, 'entity': 'PER.NAM', 'text': ['é‚“', 'è¶…'
            ]
        }
    ]
]
```

### ğŸ“š Citation
```bibtex
@inproceedings{lai-etal-2025-improving,
    title = "Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations",
    author = "Lai, Peichao  and
      Gan, Jiaxin  and
      Ye, Feiyang  and
      Zhang, Wentao  and
      Fu, Fangcheng  and
      Wang, Yilei  and
      Cui, Bin",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.288/",
    pages = "5666--5685",
    ISBN = "979-8-89176-332-6",
    abstract = "Sequence labeling remains a significant challenge in low-resource, domain-specific scenarios, particularly for character-dense languages. Existing methods primarily focus on enhancing model comprehension and improving data diversity to boost performance. However, these approaches still struggle with inadequate model applicability and semantic distribution biases in domain-specific contexts. To overcome these limitations, we propose a novel framework that combines an LLM-based knowledge enhancement workflow with a span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model. Our workflow employs explanation prompts to generate precise contextual interpretations of target entities, effectively mitigating semantic biases and enriching the model{'}s contextual understanding. The KnowFREE model further integrates extension label features, enabling efficient nested entity extraction without relying on external knowledge during inference. Experiments on multiple domain-specific sequence labeling datasets demonstrate that our approach achieves state-of-the-art performance, effectively addressing the challenges posed by low-resource settings."
}
```
